from bs4 import BeautifulSoup
import unittest
import requests

# PART 1


def get_nps_homepage_and_state_html(state_str):
    try:
        nps_gov_file = open("nps_gov_data.html", "r")
        html = nps_gov_file.read()
        nps_gov_file.close()
        print1 = "NPS homepage HTML file already exists."
        print2 = "  (This isn't a fruitful function.)"
        print(print1 + print2)
    except:
        nps_gov_home = "https://www.nps.gov/index.htm"
        response = requests.get(nps_gov_home)
        html = response.text
        nps_gov_file = open("nps_gov_data.html", "w")
        nps_gov_file.write(html)
        nps_gov_file.close()
        print3 = "NPS homepage HTML file written."
        print4 = "  (This isn't a fruitful function.)"
        print(print3 + print4)
    nps_gov_soup = BeautifulSoup(html, "html.parser")
    state_str_lc = state_str.lower()
    state_str_lc_no_spaces = state_str_lc.replace(" ", "_")
    state_data_suffix = "_data.html"
    state_data_html = state_str_lc_no_spaces + state_data_suffix
    try:
        state_data_file = open(state_data_html, "r")
        html = state_data_file.read()
        state_data_file.close()
        print5 = "NPS state HTML file already exists."
        print6 = "  (This isn't a fruitful function.)"
        print(print5 + print6)
    except:
        dropdown_class = "dropdown-menu SearchBar-keywordSearch"
        dropdown = nps_gov_soup.find("ul", {"class": dropdown_class})
        for state_li in dropdown.find_all("li"):
            state_link_text = state_li.find("a").text
            if state_link_text == state_str:
                state_link_href = state_li.find("a").get("href")
                nps_gov_prefix = "https://www.nps.gov"
                nps_state = nps_gov_prefix + state_link_href
                response = requests.get(nps_state)
                html = response.text
                nps_state_file = open(state_data_html, "w")
                nps_state_file.write(html)
                nps_state_file.close()
        print7 = "NPS state HTML file written."
        print8 = "  (This isn't a fruitful function.)"
        print(print7 + print8)

get_nps_homepage_and_state_html("Arkansas")
get_nps_homepage_and_state_html("California")
get_nps_homepage_and_state_html("Michigan")

# PART 2


class NationalSite(object):
    def __init__(self, site_soup):
        self.name = site_soup.find("h3").contents[0].text
        if site_soup.find("h4").text:
            self.location = site_soup.find("h4").text
        else:
            self.location = ""
        if site_soup.find("h2").text:
            self.type = site_soup.find("h2").text
        else:
            self.type = None
        if site_soup.find("p").text:
            self.description = site_soup.find("p").text.strip()
        else:
            self.description = ""
        all_links = site_soup.find_all("a")
        for link in all_links:
            if "basicinfo" in link.get("href"):
                    basic_info_link = link.get("href")
        self.basic_info_link = basic_info_link

    def __str__(self):
        return self.name + " | " + self.location

    def __contains__(self, input_str):
        return input_str in self.name

    def get_mailing_address(self):
        site_name_lower = self.name.lower()
        site_name_lower_underscore = site_name_lower.replace(" ", "_")
        basic_info_html_suffix = "_basic_info.html"
        html_name = site_name_lower_underscore + basic_info_html_suffix
        try:  # SHOULD'VE CACHED IN ONE JSON FILE, NOT MANY HTML FILES
            site_basic_info_file = open(html_name, "r")
            html = site_basic_info_file.read()
            site_basic_info_file.close()
        except:
            response = requests.get(self.basic_info_link)
            html = response.text
            site_basic_info_file = open(html_name, "w")
            site_basic_info_file.write(html)
            site_basic_info_file.close()
        info_soup = BeautifulSoup(html, "html.parser")
        address_str = ""
        try:
            address = info_soup.find("div", {"itemprop": "address"})
            address_lst = []
            address_children = address.contents
            for address_child in address_children:
                try:
                    to_add = address_child.text.strip()
                    if to_add != "":
                        address_lst.append(to_add)
                except:
                    pass
            for address_element in address_lst[:-1]:
                address_str = address_str + address_element + " / "
            address_str = address_str + address_lst[-1]
        except:
            pass
        return address_str

        # A cleaner way of accomplishing lines 109-120
        # (using .find_all() instead of .contents) is below,
        # but the addresses that it generates have repeats,
        # while those generated by lines 109-120 do not:

            # address_spans = address.find_all("span")
            # for address_span in address_spans[:-1]:
            #    address_str = address_str + address_span.text.strip() + " / "
            # address_str = address_str + address_spans[-1].text.strip()

# PART 3


def create_state_nationalsites_list(state_str):
    state_str_lc = state_str.lower()
    state_str_lc_no_spaces = state_str_lc.replace(" ", "_")
    state_data_suffix = "_data.html"
    state_data_html = state_str_lc_no_spaces + state_data_suffix
    state_data_file = open(state_data_html, "r")
    html = state_data_file.read()
    state_data_file.close()
    state_sites_list = []
    state_soup = BeautifulSoup(html, "html.parser")
    state_sites_ul = state_soup.find("ul", {"id": "list_parks"})
    for state_site_li in state_sites_ul.children:
        try:
            site_object = NationalSite(state_site_li)
            state_sites_list.append(site_object)
        except:
            pass
    return state_sites_list

arkansas_natl_sites = create_state_nationalsites_list("Arkansas")
california_natl_sites = create_state_nationalsites_list("California")
michigan_natl_sites = create_state_nationalsites_list("Michigan")

# PART 4


def write_site_objects_list_to_csv(list_of_site_objects, file_name):
    try:
        infile = open(file_name, "r")
        infile.close()
        print("CSV file already exists.  (This isn't a fruitful function.)")
    except:
        outfile = open(file_name, "w")
        outfile.write("NAME, LOCATION, TYPE, ADDRESS, DESCRIPTION\n")
        for site_object in list_of_site_objects:
            site_name = site_object.name
            if site_object.location:
                site_location = site_object.location
            else:
                site_location != ""
            if site_object.type:
                site_type = site_object.type
            else:
                site_type = "None"
            if site_object.get_mailing_address() != "":
                site_address = site_object.get_mailing_address()
            else:
                site_address = "None"
            if site_object.description:
                site_description = site_object.description.replace('"', "''")
            else:
                site_description = "None"
            outfile.write('"{}",'.format(site_name))
            outfile.write('"{}",'.format(site_location))
            outfile.write('"{}",'.format(site_type))
            outfile.write('"{}",'.format(site_address))
            outfile.write('"{}"'.format(site_description))
            outfile.write("\n")
        outfile.close()
        print("CSV file written.  (This isn't a fruitful function.)")

csv_ar = "arkansas.csv"
csv_ca = "california.csv"
csv_mi = "michigan.csv"
ar_csv = write_site_objects_list_to_csv(arkansas_natl_sites, csv_ar)
ca_csv = write_site_objects_list_to_csv(california_natl_sites, csv_ca)
mi_csv = write_site_objects_list_to_csv(michigan_natl_sites, csv_mi)
